Em Ä‘Ã£ káº¿t há»£p `Selenium` Ä‘á»ƒ xá»­ lÃ½ cÃ¡c trang web Ä‘á»™ng vÃ  `BeautifulSoup` Ä‘á»ƒ bÃ³c tÃ¡ch, trÃ­ch xuáº¥t dá»¯ liá»‡u tá»« HTML. 

NgoÃ i ra, Ä‘á»ƒ trÃ¡nh bá»‹ cháº·n bá»Ÿi lá»—i 429, em Ä‘Ã£ dÃ¹ng thÆ° viá»‡n `time` táº¡o khoáº£ng nghá»‰ giá»¯a cÃ¡c yÃªu cáº§u, giÃºp quÃ¡ trÃ¬nh crawl á»•n Ä‘á»‹nh hÆ¡n.

<aside>
ğŸ’¡

LÆ°u Ã½

- Äá»ƒ **Ä‘Æ¡n giáº£n hÃ³a** vÃ  giÃºp má»i ngÆ°á»i dá»… hÃ¬nh dung luá»“ng xá»­ lÃ½ chÃ­nh, em Ä‘Ã£ **áº©n Ä‘i cÃ¡c lá»‡nh** `time.sleep()` trong pháº§n giáº£i thÃ­ch code
</aside>


Em sáº½ chia bÃ i toÃ¡n crawl data ra thÃ nh cÃ¡c pháº§n nhá» Ä‘á»ƒ má»i ngÆ°á»i cÃ³ thá»ƒ hiá»ƒu lá»‘i tÆ° duy vÃ  hÆ°á»›ng lÃ m cá»§a em

### Task 1: VÃ o website TopCV

Äá»ƒ Ä‘iá»u khiá»ƒn trÃ¬nh duyá»‡t, em sá»­ dá»¥ng **ChromeDriver** Ä‘á»ƒ `Selenium` tÆ°Æ¡ng tÃ¡c vá»›i Google Chrome. 

VÃ¬ TopCV lÃ  má»™t trang web cÃ´ng khai khÃ´ng yÃªu cáº§u Ä‘Äƒng nháº­p, viá»‡c truy cáº­p ban Ä‘áº§u ráº¥t Ä‘Æ¡n giáº£n, chá»‰ cáº§n thá»±c thi lá»‡nh `.get()` Ä‘á»ƒ táº£i trang

```python
driver = webdriver.Chrome()
url = 'https://www.topcv.vn/'
driver.get(url)
```

### Task 2: Search loáº¡i Job muá»‘n Crawl

Khi muá»‘n tÄƒng tÃ¡c vá»›i cÃ¡c váº­t thá»ƒ, em sáº½ sá»­ dá»¥ng lá»‡nh `.find_element()` Ä‘á»ƒ xÃ¡c Ä‘á»‹nh vá»‹ trÃ­ váº­t thá»ƒ

> *Thá»±c cháº¥t á»Ÿ Ä‘Ã¢y Ä‘á»ƒ ngáº¯n gá»n code thÃ¬ cÃ³ thá»ƒ dÃ¹ng luÃ´n `.find_element(By.ID, 'keyword')`*
> 

â†’ Tá»« Ä‘Ã³ chá»n cÃ¡c lá»‡nh tÆ°Æ¡ng tÃ¡c phÃ¹ há»£p

```python
finding = 'Data Analyst'

search = driver.find_element(By.XPATH, '//*[@id="keyword"]')
search.send_keys(finding) 
search.send_keys(Keys.RETURN) 
```

### Task 3: Má»Ÿ URLs cá»§a tá»«ng tin tuyá»ƒn dá»¥ng

Sau khi tÃ¬m kiáº¿m, trang káº¿t quáº£ sáº½ hiá»ƒn thá»‹ má»™t danh sÃ¡ch nhiá»u tin tuyá»ƒn dá»¥ng. Nhiá»‡m vá»¥ á»Ÿ bÆ°á»›c nÃ y lÃ  láº¥y link (URL) chi tiáº¿t cá»§a tá»«ng tin Ä‘Ã³ vÃ  lÆ°u láº¡i.

**FUNCTION 1: TrÃ­ch xuáº¥t toÃ n bá»™ URL trong 1 trang** 

CÃ¡ch tiáº¿p cáº­n cá»§a em khÃ¡ tÆ°Æ¡ng tá»± Task 2, nhÆ°ng cÃ³ hai Ä‘iá»ƒm khÃ¡c biá»‡t chÃ­nh:

- **Sá»­ dá»¥ng `find_elements()`:** vÃ¬ cÃ³ nhiá»u tin tuyá»ƒn dá»¥ng cáº§n láº¥y, em sá»­ dá»¥ng phÆ°Æ¡ng thá»©c `find_elements()`  Ä‘á»ƒ tÃ¬m **táº¥t cáº£** cÃ¡c pháº§n tá»­ khá»›p vá»›i Ä‘iá»u kiá»‡n vÃ  tráº£ vá» má»™t List URL
- **Äá»‹nh vá»‹ báº±ng `CLASS`:** em nháº­n tháº¥y má»—i tin tuyá»ƒn dá»¥ng trÃªn trang Ä‘á»u Ä‘Æ°á»£c bao bá»c trong má»™t khá»‘i HTML cÃ³ chung má»™t `class` nÃªn sá»­ dá»¥ng `By.CSS_SELECTOR` Ä‘á»ƒ Ä‘á»‹nh vá»‹.

> *Trong trÆ°á»ng há»£p nÃ y, cÃ³ thá»ƒ dÃ¹ng `.find_element(By.CLASS, '')` nhÆ°ng sáº½ dá»… bá»‹ lá»—i data trÃ­ch xuáº¥t do cÃ¡c `class` Ä‘Æ°á»£c tÃ¡i sá»­ dá»¥ng chung trÃªn trang*
> 

```python
def get_job_url() : 
    job_links = driver.find_elements(By.CSS_SELECTOR, ".job-item-search-result")
    job_list = []

    for job in job_links:
        a = job.find_element(By.CSS_SELECTOR, ".title a")
        full_url = a.get_attribute('href')
        # trimmed_url = full_url.split('.html')[0]
        job_list.append(full_url)

    return job_list
```

**FUNCTION 2: TrÃ­ch xuáº¥t URL trong toÃ n bá»™ trang** 

Dá»¯ liá»‡u tuyá»ƒn dá»¥ng thÆ°á»ng Ä‘Æ°á»£c chia thÃ nh nhiá»u trang. Äá»ƒ cÃ³ thá»ƒ thu tháº­p toÃ n bá»™ thÃ´ng tin mÃ  khÃ´ng bá» sÃ³t, scraper cáº§n cÃ³ kháº£ nÄƒng tá»± Ä‘á»™ng chuyá»ƒn qua cÃ¡c trang tiáº¿p theo.

Em táº¡o má»™t vÃ²ng láº·p (`while loop`) Ä‘á»ƒ liÃªn tá»¥c thá»±c hiá»‡n chu trÃ¬nh sau:

1. **Thu tháº­p dá»¯ liá»‡u trang hiá»‡n táº¡i:** gá»i láº¡i hÃ m `get_job_url()` 
2. **TÃ¬m nÃºt Next:** dÃ¹ng `try-except` Ä‘á»ƒ tÃ¬m kiáº¿m nÃºt Next náº¿u trang khÃ´ng cÃ³ nÃºt Next â†’ tá»©c lÃ  trang cuá»‘i cÃ¹ng â†’  `break` ngáº¯t vÃ²ng láº·p

```python
def get_url_allpage():    
    URL_all_page = []

    while True:
        URL_one_page = get_job_url()
        URL_all_page += URL_one_page

        next_buttons = driver.find_elements(By.CSS_SELECTOR, "a[rel='next']")       
        if not next_buttons:
            break  

        next_button = next_buttons[0]
        driver.execute_script("arguments[0].click();", next_button)
        sleep(3)
        
    return URL_all_page
```

### Task 4: Crawl Data + extract vÃ o file CSV

ÄÃ¢y lÃ  bÆ°á»›c cuá»‘i cÃ¹ng vÃ  cÅ©ng lÃ  bÆ°á»›c cá»‘t lÃµi cá»§a quÃ¡ trÃ¬nh thu tháº­p dá»¯ liá»‡u. Vá»›i danh sÃ¡ch URL Ä‘Ã£ cÃ³ tá»« cÃ¡c bÆ°á»›c trÆ°á»›c, em sáº½ thá»±c hiá»‡n má»™t quy trÃ¬nh tá»± Ä‘á»™ng Ä‘á»ƒ "Ä‘á»c" ná»™i dung cá»§a tá»«ng trang.

> TopCV
> 

Em sáº½ chia 1 page tuyá»ƒn dá»¥ng ra lÃ m 4 section nhá» cho viá»‡c extract thÃ´ng tin

1. header_div: pháº§n header trÃªn cÃ¹ng
2. company_div: pháº§n thÃ´ng tin cÃ´ng ty á»Ÿ gÃ³c trÃªn bÃªn pháº£i
3. skill_div: Ä‘Ã¢y lÃ  pháº§n khoanh Ä‘á» á»Ÿ phÃ­a dÆ°á»›i company_div nhÆ°ng do em chá»‰ extract má»—i **Skills** nÃªn gá»i lÃ  skill_div
4. detail_div: pháº§n chi tiáº¿t tuyá»ƒn dá»¥ng

![image.png](attachment:971171ec-9815-4e98-ad4f-fe04ba8882c7:image.png)

Luá»“ng xá»­ lÃ½ Ä‘Æ°á»£c thá»±c hiá»‡n nhÆ° sau:

- **Táº¡o vÃ²ng láº·p:** má»™t vÃ²ng láº·p `for` Ä‘Æ°á»£c táº¡o Ä‘á»ƒ duyá»‡t qua tá»«ng URL trong danh sÃ¡ch Ä‘Ã£ thu tháº­p.
- **Táº£i trang vÃ  PhÃ¢n tÃ­ch HTML:** em dÃ¹ng`Selenium`Ä‘á»ƒ truy cáº­p vÃ  táº£i toÃ n bá»™ ná»™i dung cá»§a trang tuyá»ƒn dá»¥ng. Rá»“i, em táº£i HTML Doc báº±ng `BeautifulSoup` Ä‘á»ƒ dá»… dÃ ng extract thÃ´ng tin.
- **TrÃ­ch xuáº¥t cÃ¡c trÆ°á»ng thÃ´ng tin:** cuá»‘i cÃ¹ng, em sá»­ dá»¥ng `.find()` vÃ  `.find_all()` Ä‘á»ƒ Ä‘á»‹nh vá»‹ chÃ­nh xÃ¡c cÃ¡c trÆ°á»ng thÃ´ng tin cáº§n thiáº¿t.

VÃ¬ pháº§n skill_div thÆ°á»ng khÃ´ng pháº£i lÃºc nÃ o thÃ´ng tin cÅ©ng cá»‘ Ä‘á»‹nh mÃ  cÃ³ thá»ƒ thay Ä‘á»•i tÃ¹y theo NhÃ  Tuyá»ƒn dá»¥ng

> HÆ°á»›ng xá»­ lÃ½
> 
1. VÃ²ng láº·p `for` vÃ  xÃ©t tá»«ng má»¥c trong skill_div 
2. Crawl data á»Ÿ má»¥c nÃ o cÃ³ **title** lÃ  â€œ*Ká»¹ nÄƒng cáº§n cÃ³*â€ vÃ  â€œ*Ká»¹ nÄƒng nÃªn cÃ³*â€ 

> Full ThÃ´ng tin
> 

![image.png](attachment:62c87e05-9ff1-4c70-9233-dcb6e66c4787:image.png)

> Thiáº¿u thÃ´ng tin
> 

![image.png](attachment:0ea9db32-eab4-4d24-94f7-f3eccb64330b:image.png)

> Code
> 

```python
for i in range(len(URL)):

    driver.get(URL[i])
    page_source = BeautifulSoup(driver.page_source, "html.parser")

    header_div = page_source.find('div', class_="job-detail__info") 
    # 1.1 Title
    job_title_div = header_div.find_all('h1', class_="job-detail__info--title")
    job_title = job_title_div[0].get_text().strip()
    # 1.2 Salary
    salary_div = header_div.find_all('div', class_="job-detail__info--section-content-value")
    salary = salary_div[0].get_text().strip()
    # 1.3 Place
    place = salary_div[1].get_text().strip()
    # 1.4 YOE
    yoe = salary_div[2].get_text().strip()

    # 2. Company Info
    company_div = page_source.find('div', class_="job-detail__box--right job-detail__company")
    company_name_div = company_div.find_all('div', class_="company-name")
    company_name = company_name_div[0].get_text().strip()
    company_no_emp = company_div.find_all('div', class_="company-value")[0].get_text().strip()
    company_industry = company_div.find_all('div', class_="company-value")[1].get_text().strip()

    # 3. Skills
    require_skill_div = ''
    add_skill_div = ''

    skill_div = page_source.find('div', class_="job-detail__box--right job-detail__body-right--item job-detail__body-right--box-category")
    if skill_div:
        titles = skill_div.find_all('div', class_="box-title")
        skill_tags = skill_div.find_all('div', class_="box-category-tags")

        for idx, title_tag in enumerate(titles):
            title_text = title_tag.get_text()

            skill_content = skill_tags[idx].get_text()

            if title_text == "Ká»¹ nÄƒng cáº§n cÃ³":
                require_skill_div = skill_content
            elif title_text == "Ká»¹ nÄƒng nÃªn cÃ³":
                add_skill_div = skill_content

    # 4. Job Detail
    detail_div = page_source.find_all('div', class_="job-description__item")
    job_description = detail_div[0].get_text().strip()
    job_requirement = detail_div[1].get_text().strip()
    job_benefit = detail_div[2].get_text().strip()
    job_schedule = detail_div[4].get_text().strip()
```

Dá»¯ liá»‡u Ä‘Æ°á»£c lÆ°u vÃ o file CSV báº±ng thÆ° viá»‡n `csv` cá»§a Python. Em cÃ³ dÃ¹ng thÃªm encoding â€˜utf-8â€™ Ä‘á»ƒ trÃ¡nh bá»‹ lá»—i khi upload ngÃ´n ngá»¯ Tiáº¿ng Viá»‡t lÃªn

```python
with open('output2.csv', 'w', newline='', encoding='utf-8-sig') as file_output:
    headers = [
		# ...
    ]
    writer = csv.DictWriter(file_output, delimiter=',', lineterminator='\n', fieldnames=headers)
    writer.writeheader()
    
    writer.writerow({
		# ...
        })
```

## 2.2 Transform
